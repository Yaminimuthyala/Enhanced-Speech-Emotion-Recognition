{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pj26CU0sXrh7",
    "outputId": "e2d438e1-bb21-4b44-e9ed-9e2bf9256bab"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UgBjeHMgKwir",
    "outputId": "6bf3a61b-b8ae-4a18-9dcb-ecb9e95ba905"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_4WAT0LX5V8"
   },
   "outputs": [],
   "source": [
    "path_to_cremad = '/content/drive/My Drive/audio_datasets/cremad/AudioWAV/'\n",
    "path_to_ravdess = '/content/drive/My Drive/audio_datasets/ravdess/'\n",
    "path_to_savee = '/content/drive/My Drive/audio_datasets/savee/ALL/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3ySShETX8sq"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# # List all WAV files in the CREMA-D directory\n",
    "# file_paths = glob.glob(os.path.join(path_to_cremad, \"*.wav\"))\n",
    "\n",
    "# # Parse the filenames and extract information\n",
    "# data = []\n",
    "# for file_path in file_paths:\n",
    "#     # Extract the filename from the path\n",
    "#     filename = os.path.basename(file_path)\n",
    "#     parts = filename.split(\"_\")\n",
    "#     actor_id = parts[0]\n",
    "#     sentence_code = parts[1]\n",
    "#     emotion_abbr = parts[2]\n",
    "\n",
    "#     # Map the abbreviations to full descriptions (you'll need to fill these based on the dataset's documentation)\n",
    "#     emotion_dict = {\n",
    "#         'ANG': 'Anger',\n",
    "#         'DIS': 'Disgust',\n",
    "#         'FEA': 'Fear',\n",
    "#         'HAP': 'Happy',\n",
    "#         'NEU': 'Neutral',\n",
    "#         'SAD': 'Sad',\n",
    "#         # ... add other emotions as necessary\n",
    "#     }\n",
    "\n",
    "#     emotion = emotion_dict.get(emotion_abbr, 'Unknown')\n",
    "\n",
    "#     # Append the data to our list\n",
    "#     data.append({\n",
    "#         'Filename': filename,\n",
    "#         'Emotion': emotion,\n",
    "#         'Dataset':'CREMA-D',\n",
    "#         'Path': file_path })\n",
    "\n",
    "# # Create a DataFrame from the data\n",
    "# df_crema = pd.DataFrame(data)\n",
    "\n",
    "# # Display the DataFrame\n",
    "# df_crema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "Iwx-yQJiQoyd",
    "outputId": "0e4ed4eb-18cd-4ac6-b42e-c502d89bc2aa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# List all WAV files in the CREMA-D directory\n",
    "file_paths = glob.glob(os.path.join(path_to_cremad, \"*.wav\"))\n",
    "\n",
    "# Parse the filenames and extract information\n",
    "data = []\n",
    "for file_path in file_paths:\n",
    "    # Extract the filename from the path\n",
    "    filename = os.path.basename(file_path)\n",
    "    parts = filename.split(\"_\")\n",
    "    actor_id = parts[0]\n",
    "    sentence_code = parts[1]\n",
    "    emotion_abbr = parts[2]\n",
    "\n",
    "    # Map the abbreviations to full descriptions (you'll need to fill these based on the dataset's documentation)\n",
    "    emotion_dict = {\n",
    "        'ANG': 'Anger',\n",
    "        'DIS': 'Disgust',\n",
    "        'FEA': 'Fear',\n",
    "        'HAP': 'Happy',\n",
    "        'NEU': 'Neutral',\n",
    "        'SAD': 'Sad',\n",
    "        # ... add other emotions as necessary\n",
    "    }\n",
    "\n",
    "    emotion = emotion_dict.get(emotion_abbr, 'Unknown')\n",
    "\n",
    "    # Determine gender based on actor_id\n",
    "    if int(actor_id) in [1002, 1003, 1004, 1006, 1007, 1008, 1009, 1010, 1012, 1013, 1018, 1020, 1021, 1024, 1025, 1028, 1029, 1030, 1037, 1043, 1046, 1047, 1049,\n",
    "          1052, 1053, 1054, 1055, 1056, 1058, 1060, 1061, 1063, 1072, 1073, 1074, 1075, 1076, 1078, 1079, 1082, 1084, 1089, 1091]:\n",
    "        gender = 'female'\n",
    "    else:\n",
    "        gender = 'male'\n",
    "\n",
    "    # Append the data to our list\n",
    "    data.append({\n",
    "        'Filename': filename,\n",
    "        'Emotion': emotion,\n",
    "        'Gender': gender,\n",
    "        'Dataset': 'CREMA-D',\n",
    "        'Path': file_path\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df_crema = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_crema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEPlQJI8Qonb"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "Rw-KTfL6QUMT",
    "outputId": "b3d4d74f-81f1-47ae-a75f-e2b44756fb7f"
   },
   "outputs": [],
   "source": [
    "# RAVDESS Emotion mapping\n",
    "emotion_map_ravdess = {\n",
    "    '01': 'Neutral', '02': 'Calm', '03': 'Happy', '04': 'Sad',\n",
    "    '05': 'Anger', '06': 'Fearful', '07': 'Disgust', '08': 'Surprise'\n",
    "}\n",
    "\n",
    "# Parse RAVDESS filenames\n",
    "ravdess_data = []\n",
    "for subdir in os.listdir(path_to_ravdess):\n",
    "    subdir_path = os.path.join(path_to_ravdess, subdir)\n",
    "    if os.path.isdir(subdir_path):\n",
    "        for filename in os.listdir(subdir_path):\n",
    "            parts = filename.split('-')\n",
    "            if len(parts) >= 7:  # Ensure the filename has the expected number of parts\n",
    "                emotion = emotion_map_ravdess.get(parts[2], 'Unknown')\n",
    "                actor_id = parts[6].replace('.wav', '')\n",
    "\n",
    "                # Determine gender based on actor ID\n",
    "                gender = \"female\" if int(actor_id) % 2 == 0 else \"male\"\n",
    "\n",
    "                ravdess_data.append({\n",
    "                    'Filename': filename,\n",
    "                    'Emotion': emotion,\n",
    "                    'Gender': gender,\n",
    "                    'Dataset': 'RAVDESS',\n",
    "                    'Path': os.path.join(subdir_path, filename)\n",
    "                })\n",
    "\n",
    "# Create DataFrame for RAVDESS\n",
    "df_ravdess = pd.DataFrame(ravdess_data)\n",
    "\n",
    "# Display DataFrame\n",
    "df_ravdess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lg2PD99FYC9h"
   },
   "outputs": [],
   "source": [
    "\n",
    "# # RAVDESS Emotion mapping\n",
    "# emotion_map_ravdess = {\n",
    "#     '01': 'Neutral', '02': 'Calm', '03': 'Happy', '04': 'Sad',\n",
    "#     '05': 'Anger', '06': 'Fearful', '07': 'Disgust', '08': 'Surprise'\n",
    "# }\n",
    "\n",
    "# # Parse RAVDESS filenames\n",
    "# ravdess_data = []\n",
    "# for subdir in os.listdir(path_to_ravdess):\n",
    "#     subdir_path = os.path.join(path_to_ravdess, subdir)\n",
    "#     if os.path.isdir(subdir_path):\n",
    "#         for filename in os.listdir(subdir_path):\n",
    "#             parts = filename.split('-')\n",
    "#             if len(parts) >= 7:  # Ensure the filename has the expected number of parts\n",
    "#                 emotion = emotion_map_ravdess.get(parts[2], 'Unknown')\n",
    "#                 actor_id = parts[6].replace('.wav', '')\n",
    "#                 ravdess_data.append({\n",
    "#                     'Filename': filename,\n",
    "#                     'Emotion': emotion,\n",
    "#                     'Dataset':'RAVDESS',\n",
    "#                     'Path': subdir_path+'/'+filename\n",
    "#                 })\n",
    "\n",
    "# # Create DataFrame\n",
    "# df_ravdess = pd.DataFrame(ravdess_data)\n",
    "\n",
    "# # Display DataFrame\n",
    "# df_ravdess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4urTjl5UKUw"
   },
   "outputs": [],
   "source": [
    "# SAVEE='/content/drive/My Drive/audio_datasets/savee/ALL/'\n",
    "# # savee dataset load\n",
    "# dir_list = os.listdir(SAVEE)\n",
    "\n",
    "# # parsing the file name to get the emotions\n",
    "# emotion=[]\n",
    "# path = []\n",
    "# for i in dir_list:\n",
    "#     if i[-8:-6]=='_a':\n",
    "#         emotion.append('male_angry')\n",
    "#     elif i[-8:-6]=='_d':\n",
    "#         emotion.append('male_disgust')\n",
    "#     elif i[-8:-6]=='_f':\n",
    "#         emotion.append('male_fear')\n",
    "#     elif i[-8:-6]=='_h':\n",
    "#         emotion.append('male_happy')\n",
    "#     elif i[-8:-6]=='_n':\n",
    "#         emotion.append('male_neutral')\n",
    "#     elif i[-8:-6]=='sa':\n",
    "#         emotion.append('male_sad')\n",
    "#     elif i[-8:-6]=='su':\n",
    "#         emotion.append('male_surprise')\n",
    "#     else:\n",
    "#         emotion.append('male_error')\n",
    "#     path.append(SAVEE + i)\n",
    "\n",
    "\n",
    "# SAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\n",
    "# SAVEE_df ['source'] = 'SAVEE'\n",
    "# SAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\n",
    "# SAVEE_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mO3QOmBUTeNq"
   },
   "outputs": [],
   "source": [
    "SAVEE='/content/drive/My Drive/audio_datasets/savee/ALL/'\n",
    "\n",
    "# SAVEE dataset load\n",
    "dir_list = os.listdir(SAVEE)\n",
    "\n",
    "# Initialize lists to store data\n",
    "filenames = []\n",
    "emotions = []\n",
    "genders = []\n",
    "datasets = []\n",
    "paths = []\n",
    "\n",
    "\n",
    "for i in dir_list:\n",
    "    # Extract emotion and gender from the filename\n",
    "    if i[-8:-6] == '_a':\n",
    "        emotion = 'Anger'\n",
    "        gender = 'male'\n",
    "    elif i[-8:-6] == '_d':\n",
    "        emotion = 'Disgust'\n",
    "        gender = 'male'\n",
    "    elif i[-8:-6] == '_f':\n",
    "        emotion = 'Fear'\n",
    "        gender = 'male'\n",
    "    elif i[-8:-6] == '_h':\n",
    "        emotion = 'Happy'\n",
    "        gender = 'male'\n",
    "    elif i[-8:-6] == '_n':\n",
    "        emotion = 'Neutral'\n",
    "        gender = 'male'\n",
    "    elif i[-8:-6] == 'sa':\n",
    "        emotion = 'Sad'\n",
    "        gender = 'male'\n",
    "    elif i[-8:-6] == 'su':\n",
    "        emotion = 'Surprise'\n",
    "        gender = 'male'\n",
    "    else:\n",
    "        emotion = 'unknown'\n",
    "        gender = 'unknown'\n",
    "\n",
    "    # Append data to lists\n",
    "    filenames.append(i)\n",
    "    emotions.append(emotion)\n",
    "    genders.append(gender)\n",
    "    datasets.append('SAVEE')\n",
    "    paths.append(os.path.join(SAVEE, i))\n",
    "\n",
    "# Create DataFrame for SAVEE\n",
    "savee_df = pd.DataFrame({\n",
    "    'Filename': filenames,\n",
    "    'Emotion': emotions,\n",
    "    'Gender': genders,\n",
    "    'Dataset': datasets,\n",
    "    'Path': paths\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "savee_df.head()\n",
    "\n",
    "df_savee=savee_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "FNBLXTyEXjcM",
    "outputId": "654bbb64-43b6-4fe2-cf07-c7a98b7d3218"
   },
   "outputs": [],
   "source": [
    "df_savee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tr2J1LN4YFZJ"
   },
   "outputs": [],
   "source": [
    "# # Path to SAVEE dataset\n",
    "# path_to_savee = '/content/drive/My Drive/audio_datasets/savee/ALL/'\n",
    "\n",
    "# # SAVEE Emotion mapping\n",
    "# emotion_map_savee = {\n",
    "#     'a': 'Anger', 'd': 'Disgust', 'f': 'Fear',\n",
    "#     'h': 'Happy', 'n': 'Neutral', 'sa': 'Sad', 'su': 'Surprise'\n",
    "# }\n",
    "\n",
    "# # Parse SAVEE filenames\n",
    "# savee_data = []\n",
    "# for filename in os.listdir(path_to_savee):\n",
    "#     # Extract the emotion code from the filename\n",
    "#     parts = filename.split('_')\n",
    "#     if len(parts) > 1:\n",
    "#         emotion_code = parts[1][:2]  # Extracting first two characters\n",
    "#         # Check if the code is a single letter (and not 'sa' or 'su')\n",
    "#         if emotion_code[0] in emotion_map_savee and emotion_code not in ['sa', 'su']:\n",
    "#             emotion_code = emotion_code[0]\n",
    "#         emotion = emotion_map_savee.get(emotion_code, 'Unknown')\n",
    "#         actor_id = parts[0]  # Actor ID is the first part\n",
    "#         savee_data.append({\n",
    "#             'Filename': filename,\n",
    "#             'Emotion': emotion,\n",
    "#             'Dataset':'SAVEE',\n",
    "#             'Path':path_to_savee+filename\n",
    "#         })\n",
    "\n",
    "# # Create DataFrame\n",
    "# df_savee = pd.DataFrame(savee_data)\n",
    "\n",
    "# # Display DataFrame\n",
    "# df_savee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1ILKavnuOPS"
   },
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_crema, df_ravdess, df_savee], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "H2z1NpMHWnJ-",
    "outputId": "8be8036e-32e6-4a32-e358-8f565eb0f6f0"
   },
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-cxYr-1muRdf"
   },
   "outputs": [],
   "source": [
    "df_final.to_csv(\"./final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04pzlSODYI3O",
    "outputId": "51c18b99-3a4f-4b8e-b643-ae01e7e0bb5b"
   },
   "outputs": [],
   "source": [
    "# Check for missing values in CREMA-D dataset\n",
    "missing_crema = df_crema.isnull().sum()\n",
    "print(\"Missing values in CREMA-D dataset:\\n\", missing_crema)\n",
    "\n",
    "# Check for missing values in RAVDESS dataset\n",
    "missing_ravdess = df_ravdess.isnull().sum()\n",
    "print(\"\\nMissing values in RAVDESS dataset:\\n\", missing_ravdess)\n",
    "\n",
    "# Check for missing values in SAVEE dataset\n",
    "missing_savee = df_savee.isnull().sum()\n",
    "print(\"\\nMissing values in SAVEE dataset:\\n\", missing_savee)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcwvVS7LYOtv",
    "outputId": "8e47cc1d-e47e-46ec-d30e-73fdfc751c14"
   },
   "outputs": [],
   "source": [
    "# Analysis for CREMA-D dataset\n",
    "print(\"\\nCREMA-D Dataset Analysis:\")\n",
    "print(\"Total files:\", len(df_crema))\n",
    "print(\"Emotion counts:\\n\", df_crema['Emotion'].value_counts())\n",
    "\n",
    "# Analysis for RAVDESS dataset\n",
    "print(\"\\nRAVDESS Dataset Analysis:\")\n",
    "print(\"Total files:\", len(df_ravdess))\n",
    "print(\"Emotion counts:\\n\", df_ravdess['Emotion'].value_counts())\n",
    "\n",
    "# Analysis for SAVEE dataset\n",
    "print(\"\\nSAVEE Dataset Analysis:\")\n",
    "print(\"Total files:\", len(df_savee))\n",
    "print(\"Emotion counts:\\n\", df_savee['Emotion'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SkuD7BF3YRx_",
    "outputId": "d4c0a9fd-7a66-430a-83b2-ec1a6058e2f8"
   },
   "outputs": [],
   "source": [
    "def check_audio_format(df, directory, expected_format=\".wav\"):\n",
    "    wrong_format_files = []\n",
    "    for filename in df['Filename']:\n",
    "        if not filename.endswith(expected_format):\n",
    "            wrong_format_files.append(filename)\n",
    "    return wrong_format_files, len(wrong_format_files)\n",
    "\n",
    "# Check audio format for each dataset\n",
    "wrong_format_crema, count_crema = check_audio_format(df_crema, path_to_cremad)\n",
    "wrong_format_ravdess, count_ravdess = check_audio_format(df_ravdess, path_to_ravdess)\n",
    "wrong_format_savee, count_savee = check_audio_format(df_savee, path_to_savee)\n",
    "\n",
    "print(\"Non-WAV files in CREMA-D:\", wrong_format_crema)\n",
    "print(\"Non-WAV files in RAVDESS:\", wrong_format_ravdess)\n",
    "print(\"Non-WAV files in SAVEE:\", wrong_format_savee)\n",
    "\n",
    "print(\"Count of non-WAV files in CREMA-D:\", count_crema)\n",
    "print(\"Count of non-WAV files in RAVDESS:\", count_ravdess)\n",
    "print(\"Count of non-WAV files in SAVEE:\", count_savee)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lnhAMrXGZuAh",
    "outputId": "16197cd5-10d8-476c-9a47-86afb2e0bc91"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "\n",
    "def plot_waveforms_spectrograms_and_audios(df, directory, title):\n",
    "    emotions = df['Emotion'].unique()\n",
    "    selected_emotions = emotions[:3]  # Select the first five emotions\n",
    "\n",
    "    for emotion in selected_emotions:\n",
    "        # Select the first file for each emotion\n",
    "        filename = df[df['Emotion'] == emotion]['Filename'].iloc[0]\n",
    "        path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            audio, sr = librosa.load(path, sr=None)\n",
    "\n",
    "            # Plot waveform\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.subplot(2, 1, 1)\n",
    "            plt.title(f'Emotion: {emotion}, File: {filename} - Waveform', fontsize=16)\n",
    "            librosa.display.waveshow(audio, sr=sr)\n",
    "\n",
    "            # Plot spectrogram\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plt.title(f'Emotion: {emotion}, File: {filename} - Spectrogram', fontsize=16)\n",
    "            S = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "            librosa.display.specshow(librosa.power_to_db(S, ref=np.max), y_axis='mel', x_axis='time')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Play the audio\n",
    "            ipd.display(ipd.Audio(audio, rate=sr))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {path}: {str(e)}\")\n",
    "\n",
    "print(\"CREMA-D\")\n",
    "# Plot waveforms, spectrograms, and audios for each dataset\n",
    "plot_waveforms_spectrograms_and_audios(df_crema, path_to_cremad, \"CREMA-D Waveforms, Spectrograms, and Audios\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vDvrFRcZY9H2",
    "outputId": "de82d936-c247-4d07-e47e-621fdca641c5"
   },
   "outputs": [],
   "source": [
    "print(\"SAVEE DATASET\")\n",
    "plot_waveforms_spectrograms_and_audios(df_savee, path_to_savee, \"SAVEE Waveforms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eLMh5NJBpFj1",
    "outputId": "89b92717-9759-400e-bc12-6c8ba7be353e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Define the number of emotions to display\n",
    "num_emotions_to_display = 3\n",
    "\n",
    "# Get a list of unique emotions in the dataset\n",
    "unique_emotions = df_ravdess['Emotion'].unique()\n",
    "\n",
    "# Randomly select 5 different emotions\n",
    "selected_emotions = random.sample(list(unique_emotions), num_emotions_to_display)\n",
    "\n",
    "# Check if the files exist in subdirectories\n",
    "missing_files = []\n",
    "print(\"RAVDESS DATASET\")\n",
    "for emotion in selected_emotions:\n",
    "    # Get the first file for each selected emotion\n",
    "    filename = df_ravdess[df_ravdess['Emotion'] == emotion]['Filename'].iloc[0]\n",
    "    # Iterate through subdirectories\n",
    "    for root, dirs, files in os.walk(path_to_ravdess):\n",
    "        # Check if the file exists in the current directory\n",
    "        file_path = os.path.join(root, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                # Load the audio file\n",
    "                audio, sr = librosa.load(file_path, sr=None)\n",
    "                # Display audio playback\n",
    "                print(f'Emotion: {emotion}, File: {filename} - Audio Playback')\n",
    "                ipd.display(ipd.Audio(audio, rate=sr))\n",
    "\n",
    "\n",
    "                # Plotting\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.title(f'Emotion: {emotion}, File: {filename} - Waveform', fontsize=16)\n",
    "                librosa.display.waveshow(audio, sr=sr)\n",
    "\n",
    "                # Plot spectrogram\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.title(f'Emotion: {emotion}, File: {filename} - Spectrogram', fontsize=16)\n",
    "                S = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "                librosa.display.specshow(librosa.power_to_db(S, ref=np.max), y_axis='mel', x_axis='time')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            break  # File found, no need to check other subdirectories\n",
    "    else:\n",
    "        # File was not found in any subdirectory\n",
    "        missing_files.append(filename)\n",
    "\n",
    "# Print missing files\n",
    "if missing_files:\n",
    "    print(\"The following files are missing:\")\n",
    "    for missing_file in missing_files:\n",
    "        print(missing_file)\n",
    "else:\n",
    "    print(\"All selected files exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "uMvJscU-wHsU",
    "outputId": "6e79ac8d-78d7-4190-986c-339bddea3454"
   },
   "outputs": [],
   "source": [
    "# Concatenate the dataframes\n",
    "combined_df = pd.concat([df_crema, df_ravdess, df_savee], ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe as a CSV file\n",
    "combined_df.to_csv('/content/drive/My Drive/audio_datasets/combined_dataset.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the combined dataframe\n",
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oPiCMEQWwSXp",
    "outputId": "3bc7b65f-338c-4185-a308-c0b54d7e7471"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the combined dataset\n",
    "combined_df = pd.read_csv('/content/drive/My Drive/audio_datasets/combined_dataset.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(combined_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZySkLffXxlj-",
    "outputId": "ebac81ab-c4b3-4732-983d-42193ae735c0"
   },
   "outputs": [],
   "source": [
    "# Print the emotion distribution\n",
    "emotion_counts = combined_df['Emotion'].value_counts()\n",
    "for emotion, count in emotion_counts.items():\n",
    "    print(f\"- {emotion}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "L-WbcjB9we4c",
    "outputId": "973c496b-67b9-4c5b-c5a9-ff240e84f209"
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of emotions\n",
    "plt.figure(figsize=(10, 6))\n",
    "combined_df['Emotion'].value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title('Emotion Distribution in the Dataset')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQiuwYJVA-Jp",
    "outputId": "93537bd2-0b71-4086-fe80-48e6dbf9b803"
   },
   "outputs": [],
   "source": [
    "pip install noisereduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "59OoYtcgY9oH",
    "outputId": "108b03f6-066f-4ca5-9560-f17b3dff9873"
   },
   "outputs": [],
   "source": [
    "import noisereduce as nr\n",
    "\n",
    "# Function to apply noise reduction and display audio\n",
    "def apply_noise_reduction_and_display(audio, sr, title):\n",
    "    # Apply noise reduction using noisereduce library\n",
    "    reduced_audio = nr.reduce_noise(y=audio, sr=sr)\n",
    "\n",
    "    # Display the original and reduced audio\n",
    "    print(f\"Original {title} Audio:\")\n",
    "    ipd.display(ipd.Audio(audio, rate=sr))\n",
    "    print(f\"Noise-Reduced {title} Audio:\")\n",
    "    ipd.display(ipd.Audio(reduced_audio, rate=sr))\n",
    "\n",
    "# Apply noise reduction to CREMA-D samples\n",
    "unique_emotions_crema = df_crema['Emotion'].unique()\n",
    "selected_emotions_crema = unique_emotions_crema[:3]  # Select the first three emotions\n",
    "\n",
    "for emotion in selected_emotions_crema:\n",
    "    # Check if there are any samples for this emotion\n",
    "    if emotion in unique_emotions_crema:\n",
    "        filename = df_crema[df_crema['Emotion'] == emotion]['Filename'].iloc[0]\n",
    "        path = os.path.join(path_to_cremad, filename)\n",
    "        audio, sr = librosa.load(path, sr=None)\n",
    "        apply_noise_reduction_and_display(audio, sr, f'CREMA-D {emotion}')\n",
    "\n",
    "\n",
    "# Apply noise reduction to RAVDESS samples\n",
    "for emotion in selected_emotions:\n",
    "    filename = df_ravdess[df_ravdess['Emotion'] == emotion]['Filename'].iloc[0]\n",
    "    for root, dirs, files in os.walk(path_to_ravdess):\n",
    "        file_path = os.path.join(root, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            audio, sr = librosa.load(file_path, sr=None)\n",
    "            apply_noise_reduction_and_display(audio, sr, f'RAVDESS {emotion}')\n",
    "            break\n",
    "# Apply noise reduction to SAVEE samples\n",
    "unique_emotions_savee = df_savee['Emotion'].unique()\n",
    "selected_emotions_savee = unique_emotions_savee[:3]  # Select the first three emotions\n",
    "\n",
    "for emotion in selected_emotions_savee:\n",
    "    # Check if there are any samples for this emotion\n",
    "    if emotion in unique_emotions_savee:\n",
    "        filename = df_savee[df_savee['Emotion'] == emotion]['Filename'].iloc[0]\n",
    "        path = os.path.join(path_to_savee, filename)\n",
    "        audio, sr = librosa.load(path, sr=None)\n",
    "        apply_noise_reduction_and_display(audio, sr, f'SAVEE {emotion}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "53t_m34La0X5",
    "outputId": "968d5863-dcd8-4d69-ce31-fa42a02a9d9d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import noisereduce as nr\n",
    "\n",
    "def plot_waveform_spectrogram_audio_before_after_noise_reduction(audio, sr, title, filename):\n",
    "    # Display original waveform and spectrogram\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.title(f'{title} - Original Waveform\\n{filename}', fontsize=8)\n",
    "    librosa.display.waveshow(audio, sr=sr)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.title(f'{title} - Original Spectrogram\\n{filename}', fontsize=8)\n",
    "    S = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "    librosa.display.specshow(librosa.power_to_db(S, ref=np.max), y_axis='mel', x_axis='time')\n",
    "\n",
    "    # Apply noise reduction\n",
    "    reduced_audio = nr.reduce_noise(y=audio, sr=sr)\n",
    "\n",
    "    # Display noise-reduced waveform and spectrogram\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.title(f'{title} - Noise-Reduced Waveform\\n{filename}', fontsize=8)\n",
    "    librosa.display.waveshow(reduced_audio, sr=sr)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.title(f'{title} - Noise-Reduced Spectrogram\\n{filename}', fontsize=8)\n",
    "    S_reduced = librosa.feature.melspectrogram(y=reduced_audio, sr=sr)\n",
    "    librosa.display.specshow(librosa.power_to_db(S_reduced, ref=np.max), y_axis='mel', x_axis='time')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Display original and noise-reduced audio\n",
    "    print(f\"Original {title} Audio - {filename}:\")\n",
    "    ipd.display(ipd.Audio(audio, rate=sr))\n",
    "    print(f\"Noise-Reduced {title} Audio - {filename}:\")\n",
    "    ipd.display(ipd.Audio(reduced_audio, rate=sr))\n",
    "\n",
    "# Apply noise reduction and display for CREMA-D samples\n",
    "unique_emotions_crema = df_crema['Emotion'].unique()\n",
    "selected_emotions_crema = unique_emotions_crema[:3]  # Select the first three emotions\n",
    "\n",
    "for emotion in selected_emotions_crema:\n",
    "    # Check if there are any samples for this emotion\n",
    "    if emotion in unique_emotions_crema:\n",
    "        filename = df_crema[df_crema['Emotion'] == emotion]['Filename'].iloc[0]\n",
    "        path = os.path.join(path_to_cremad, filename)\n",
    "        audio, sr = librosa.load(path, sr=None)\n",
    "        plot_waveform_spectrogram_audio_before_after_noise_reduction(audio, sr, f'CREMA-D {emotion}', filename)\n",
    "\n",
    "# Apply noise reduction and display for RAVDESS samples\n",
    "for emotion in selected_emotions:\n",
    "    filename = df_ravdess[df_ravdess['Emotion'] == emotion]['Filename'].iloc[0]\n",
    "    for root, dirs, files in os.walk(path_to_ravdess):\n",
    "        file_path = os.path.join(root, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            audio, sr = librosa.load(file_path, sr=None)\n",
    "            plot_waveform_spectrogram_audio_before_after_noise_reduction(audio, sr, f'RAVDESS {emotion}', filename)\n",
    "            break\n",
    "\n",
    "# Apply noise reduction and display for SAVEE samples\n",
    "unique_emotions_savee = df_savee['Emotion'].unique()\n",
    "selected_emotions_savee = unique_emotions_savee[:3]  # Select the first three emotions\n",
    "\n",
    "for emotion in selected_emotions_savee:\n",
    "    # Check if there are any samples for this emotion\n",
    "    if emotion in unique_emotions_savee:\n",
    "        filename = df_savee[df_savee['Emotion'] == emotion]['Filename'].iloc[0]\n",
    "        path = os.path.join(path_to_savee, filename)\n",
    "        audio, sr = librosa.load(path, sr=None)\n",
    "        plot_waveform_spectrogram_audio_before_after_noise_reduction(audio, sr, f'SAVEE {emotion}', filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5os4HzOglKms"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to plot waveform and spectrogram\n",
    "def plot_waveform_spectrogram(audio, sr, title):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(title + ' - Waveform')\n",
    "    librosa.display.waveshow(audio, sr=sr)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(title + ' - Spectrogram')\n",
    "    S = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "    librosa.display.specshow(librosa.power_to_db(S, ref=np.max), y_axis='mel', x_axis='time')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load the \"Surprise\" emotion audio files\n",
    "emotion = \"Surprise\"\n",
    "filenames = df_crema[df_crema['Emotion'] == emotion]['Filename']\n",
    "output_path = \"/path/to/save/generated/samples\"  # Replace with the actual output path\n",
    "\n",
    "# Data augmentation functions\n",
    "def add_noise(audio, noise_level=0.005):\n",
    "    noise = np.random.normal(0, noise_level, len(audio))\n",
    "    noisy_audio = audio + noise\n",
    "    return noisy_audio\n",
    "\n",
    "def time_stretch(audio, rate=1.2):\n",
    "    stretched_audio = librosa.effects.time_stretch(audio, rate=rate)\n",
    "    return stretched_audio\n",
    "\n",
    "def pitch_shift(audio, n_steps=2):\n",
    "    shifted_audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)\n",
    "    return shifted_audio\n",
    "\n",
    "def time_shift(audio, shift_factor=0.2):\n",
    "    shift_samples = int(len(audio) * shift_factor)\n",
    "    if shift_samples >= 0:\n",
    "        shifted_audio = np.pad(audio[:-shift_samples], (shift_samples, 0), mode='constant')\n",
    "    else:\n",
    "        shifted_audio = np.pad(audio[-shift_samples:], (0, -shift_samples), mode='constant')\n",
    "    return shifted_audio\n",
    "\n",
    "# Define the number of additional samples to generate\n",
    "num_samples_to_generate = 1271  # Adjust this number as needed\n",
    "\n",
    "for filename in filenames:\n",
    "    path = os.path.join(path_to_cremad, filename)\n",
    "    try:\n",
    "        audio, sr = librosa.load(path, sr=None)\n",
    "\n",
    "        # Apply data augmentation and save generated samples\n",
    "        for i in range(num_samples_to_generate):\n",
    "            # Choose a random augmentation technique\n",
    "            augmentation_choice = np.random.choice(['noise', 'stretch', 'pitch', 'shift'])\n",
    "\n",
    "            # Apply the selected augmentation\n",
    "            if augmentation_choice == 'noise':\n",
    "                augmented_audio = add_noise(audio)\n",
    "            elif augmentation_choice == 'stretch':\n",
    "                augmented_audio = time_stretch(audio)\n",
    "            elif augmentation_choice == 'pitch':\n",
    "                augmented_audio = pitch_shift(audio)\n",
    "            elif augmentation_choice == 'shift':\n",
    "                augmented_audio = time_shift(audio)\n",
    "\n",
    "            # Save the augmented audio\n",
    "            output_filename = f\"{filename[:-4]}_{augmentation_choice}_{i}.wav\"\n",
    "            output_filepath = os.path.join(output_path, output_filename)\n",
    "            librosa.output.write_wav(output_filepath, augmented_audio, sr=sr)\n",
    "\n",
    "            print(f\"Generated: {output_filepath}\")  # Print when a sample is generated\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FtXtC-IVcgsv",
    "outputId": "06bd042d-41a6-4cb3-f029-61dedadab23d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "# Function to plot waveform and spectrogram\n",
    "def plot_waveform_spectrogram_audio(audio, sr, title, filename, augmented_audio=None):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.title(f'{title} - Original Waveform\\n{filename}', fontsize=8)\n",
    "    librosa.display.waveshow(audio, sr=sr)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.title(f'{title} - Original Spectrogram\\n{filename}', fontsize=8)\n",
    "    S = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "    librosa.display.specshow(librosa.power_to_db(S, ref=np.max), y_axis='mel', x_axis='time')\n",
    "\n",
    "    if augmented_audio is not None:\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.title(f'{title} - Augmented Waveform\\n{filename}', fontsize=8)\n",
    "        librosa.display.waveshow(augmented_audio, sr=sr)\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.title(f'{title} - Augmented Spectrogram\\n{filename}', fontsize=8)\n",
    "        S_augmented = librosa.feature.melspectrogram(y=augmented_audio, sr=sr)\n",
    "        librosa.display.specshow(librosa.power_to_db(S_augmented, ref=np.max), y_axis='mel', x_axis='time')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load the first five audio files from CREMA-D\n",
    "selected_emotions = df_crema['Emotion'].unique()[:3]\n",
    "\n",
    "for emotion in selected_emotions:\n",
    "    # Select the first file for each emotion\n",
    "    filename = df_crema[df_crema['Emotion'] == emotion]['Filename'].iloc[0]\n",
    "    path = os.path.join(path_to_cremad, filename)\n",
    "    try:\n",
    "        audio, sr = librosa.load(path, sr=None)\n",
    "\n",
    "        # Data augmentation functions\n",
    "        def add_noise(audio, noise_level=0.005):\n",
    "            noise = np.random.normal(0, noise_level, len(audio))\n",
    "            noisy_audio = audio + noise\n",
    "            return noisy_audio\n",
    "\n",
    "        def time_stretch(audio, rate=1.2):\n",
    "            stretched_audio = librosa.effects.time_stretch(audio, rate=rate)\n",
    "            return stretched_audio\n",
    "\n",
    "        def pitch_shift(audio, n_steps=2):\n",
    "            shifted_audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)\n",
    "            return shifted_audio\n",
    "\n",
    "        def time_shift(audio, shift_factor=0.2):\n",
    "            shift_samples = int(len(audio) * shift_factor)\n",
    "            if shift_samples >= 0:\n",
    "                shifted_audio = np.pad(audio[:-shift_samples], (shift_samples, 0), mode='constant')\n",
    "            else:\n",
    "                shifted_audio = np.pad(audio[-shift_samples:], (0, -shift_samples), mode='constant')\n",
    "            return shifted_audio\n",
    "\n",
    "        # Apply data augmentation and visualize results\n",
    "\n",
    "        # 1. Noise Injection\n",
    "        noisy_audio = add_noise(audio)\n",
    "        plot_waveform_spectrogram_audio(audio, sr, f'Original - Emotion: {emotion}', filename, augmented_audio=noisy_audio)\n",
    "        ipd.display(ipd.Audio(audio, rate=sr))  # Display the original audio\n",
    "        ipd.display(ipd.Audio(noisy_audio, rate=sr))  # Display the noisy audio\n",
    "\n",
    "        # 2. Time Stretching\n",
    "        stretched_audio = time_stretch(audio, rate=1.2)\n",
    "        plot_waveform_spectrogram_audio(audio, sr, f'Original - Emotion: {emotion}', filename, augmented_audio=stretched_audio)\n",
    "        ipd.display(ipd.Audio(stretched_audio, rate=sr))  # Display the stretched audio\n",
    "\n",
    "        # 3. Pitch Shifting\n",
    "        shifted_audio = pitch_shift(audio, n_steps=2)\n",
    "        plot_waveform_spectrogram_audio(audio, sr, f'Original - Emotion: {emotion}', filename, augmented_audio=shifted_audio)\n",
    "        ipd.display(ipd.Audio(shifted_audio, rate=sr))  # Display the pitch shifted audio\n",
    "\n",
    "        # 4. Time Shifting\n",
    "        shifted_audio = time_shift(audio, shift_factor=0.2)\n",
    "        plot_waveform_spectrogram_audio(audio, sr, f'Original - Emotion: {emotion}', filename, augmented_audio=shifted_audio)\n",
    "        ipd.display(ipd.Audio(shifted_audio, rate=sr))  # Display the time shifted audio\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAsBIGg5YkuG"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNPwdRXRYllY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "17dQcQpQlDdk",
    "outputId": "1ae05e82-47f1-4abd-f562-da8a5fafb16c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to apply data augmentation, display audio, and plot spectrograms\n",
    "def apply_data_augmentation_and_display(audio, sr, emotion):\n",
    "    # Display the original audio\n",
    "    print(f\"Original {emotion} Audio:\")\n",
    "    ipd.display(ipd.Audio(audio, rate=sr))\n",
    "\n",
    "    # Plot the spectrogram of the original audio\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f'{emotion} - Original Spectrogram')\n",
    "    S = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "    librosa.display.specshow(librosa.power_to_db(S, ref=np.max), y_axis='mel', x_axis='time')\n",
    "\n",
    "    # Apply data augmentation techniques\n",
    "    augmented_audio = audio.copy()\n",
    "\n",
    "    # 1. Add Noise\n",
    "    noise_level = 0.005  # You can adjust the noise level as needed\n",
    "    noise = np.random.normal(0, noise_level, len(augmented_audio))\n",
    "    augmented_audio += noise\n",
    "\n",
    "    # 2. Time-Stretch\n",
    "    time_stretch_factor = 1.2  # You can adjust the time-stretch factor as needed\n",
    "    augmented_audio = librosa.effects.time_stretch(augmented_audio, rate=time_stretch_factor)\n",
    "\n",
    "    # 3. Pitch-Shift\n",
    "    pitch_shift_steps = 2  # You can adjust the pitch shift steps as needed\n",
    "    augmented_audio = librosa.effects.pitch_shift(augmented_audio, sr=sr, n_steps=pitch_shift_steps)\n",
    "\n",
    "    # Display the augmented audio\n",
    "    print(f\"Augmented {emotion} Audio:\")\n",
    "    ipd.display(ipd.Audio(augmented_audio, rate=sr))\n",
    "\n",
    "    # Plot the spectrogram of the augmented audio\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f'{emotion} - Augmented Spectrogram')\n",
    "    S_augmented = librosa.feature.melspectrogram(y=augmented_audio, sr=sr)\n",
    "    librosa.display.specshow(librosa.power_to_db(S_augmented, ref=np.max), y_axis='mel', x_axis='time')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define the emotions you want to process\n",
    "emotions_to_process = ['Fearful', 'Surprise', 'Calm']\n",
    "\n",
    "# Iterate through the DataFrame to process the selected emotions\n",
    "for emotion in emotions_to_process:\n",
    "    # Get a random audio file path for the specified emotion from the DataFrame\n",
    "    random_audio_row = df_final[df_final['Emotion'] == emotion].sample(1)\n",
    "    audio_path = random_audio_row['Path'].values[0]\n",
    "\n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "    # Apply data augmentation, display the audio, and plot spectrograms\n",
    "    apply_data_augmentation_and_display(audio, sr, emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-MV4hVLZ7jm",
    "outputId": "54c7ec3c-2134-4678-93dd-0e3121882a84"
   },
   "outputs": [],
   "source": [
    "df_final.Emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JbhNx9G6ODAX",
    "outputId": "6701e928-e19c-49fb-8477-f30b1d2e5124"
   },
   "outputs": [],
   "source": [
    "!apt-get install rubberband-cli\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJJ1XEPXRhSc",
    "outputId": "385c8959-74ee-4025-e1ab-b00ab4055b97"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf  # Use soundfile for audio file saving\n",
    "import pyrubberband  # Import pyrubberband for audio processing\n",
    "\n",
    "# Define a function to perform time stretching on audio using pyrubberband\n",
    "def stretch_audio(audio, stretch_factor=0.8, sample_rate=44100):\n",
    "    stretched_audio = pyrubberband.time_stretch(audio, sample_rate, stretch_factor)\n",
    "    return stretched_audio\n",
    "\n",
    "# Define a function to perform pitch shifting on audio using pyrubberband\n",
    "def pitch_audio(audio, pitch_factor=2, sample_rate=44100):\n",
    "    pitched_audio = pyrubberband.pitch_shift(audio, sample_rate, pitch_factor)\n",
    "    return pitched_audio\n",
    "\n",
    "# Define a function to perform data augmentation using pyrubberband\n",
    "def augment_audio(audio, sample_rate, gender, emotion, filename, save_path):\n",
    "    # Noise augmentation\n",
    "    noise_audio = audio + 0.005 * np.random.randn(len(audio))\n",
    "    save_filename = f\"{filename}_noise.wav\"\n",
    "    sf.write(os.path.join(save_path, save_filename), noise_audio, sample_rate)\n",
    "\n",
    "    # Time stretching augmentation\n",
    "    stretch_factor = 1.2  # Adjust the stretching factor as needed\n",
    "    stretched_audio = stretch_audio(audio, stretch_factor, sample_rate)\n",
    "    save_filename = f\"{filename}_stretch.wav\"\n",
    "    sf.write(os.path.join(save_path, save_filename), stretched_audio, sample_rate)\n",
    "\n",
    "    # Pitch shifting augmentation\n",
    "    pitch_factor = 2  # Adjust the pitch factor as needed\n",
    "    pitched_audio = pitch_audio(audio, pitch_factor, sample_rate)\n",
    "    save_filename = f\"{filename}_pitch.wav\"\n",
    "    sf.write(os.path.join(save_path, save_filename), pitched_audio, sample_rate)\n",
    "\n",
    "    # Speed and pitch augmentation (Combining time stretch and pitch shift)\n",
    "    speed_pitch_factor = 1.5  # Adjust the factor as needed\n",
    "    speed_pitch_audio = stretch_audio(pitched_audio, speed_pitch_factor, sample_rate)\n",
    "    save_filename = f\"{filename}_speedpitch.wav\"\n",
    "    sf.write(os.path.join(save_path, save_filename), speed_pitch_audio, sample_rate)\n",
    "\n",
    "# Load df_final or df_features (whichever you want to augment)\n",
    "# Replace 'your_dataframe_here' with your actual DataFrame\n",
    "df = df_final\n",
    "\n",
    "# Filter the DataFrame to include only \"fearful,\" \"calm,\" and \"surprise\" emotions\n",
    "filtered_df = df[df['Emotion'].isin(['Fearful', 'Calm', 'Surprise'])]\n",
    "\n",
    "# Create a new DataFrame for augmented data\n",
    "augmented_df = pd.DataFrame(columns=filtered_df.columns)\n",
    "\n",
    "# Path to the folder where augmented audio files will be saved\n",
    "augmentation_save_path = '/content/drive/My Drive/audio_datasets/augmented_data'\n",
    "\n",
    "# Loop through the rows of the filtered DataFrame\n",
    "for index, row in tqdm(filtered_df.iterrows(), total=len(filtered_df), desc=\"Augmenting Data\"):\n",
    "    path = row['Path']\n",
    "    gender = row['Gender']\n",
    "    emotion = row['Emotion']\n",
    "    filename = os.path.basename(path).split('.')[0]  # Extract the filename without extension\n",
    "\n",
    "    try:\n",
    "        audio, sample_rate = sf.read(path)\n",
    "        save_filename = f\"{filename}_original.wav\"\n",
    "        sf.write(os.path.join(augmentation_save_path, save_filename), audio, sample_rate)\n",
    "\n",
    "        # Augment the audio and save\n",
    "        augment_audio(audio, sample_rate, gender, emotion, filename, augmentation_save_path)\n",
    "\n",
    "        # Add the original and augmented data to the new DataFrame\n",
    "        augmented_df = augmented_df.append(row)\n",
    "\n",
    "        # Add augmented rows to the DataFrame with appropriate filenames\n",
    "        for augmentation_type in ['noise', 'stretch', 'pitch', 'speedpitch']:\n",
    "            save_filename = f\"{filename}_{augmentation_type}.wav\"\n",
    "            augmented_row = row.copy()\n",
    "            augmented_row['Path'] = os.path.join(augmentation_save_path, save_filename)\n",
    "            augmented_df = augmented_df.append(augmented_row)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {str(e)}\")\n",
    "\n",
    "# Save the augmented DataFrame to a CSV file\n",
    "augmented_df.to_csv('augmented_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1117
    },
    "id": "63ap0_E7gmFn",
    "outputId": "e101c909-24e6-4b36-ceb6-f5af911487cf"
   },
   "outputs": [],
   "source": [
    "augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvHz405VhhzF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have two DataFrames, df1 and df2\n",
    "\n",
    "# Concatenate them along rows (vertically)\n",
    "concatenated_df = pd.concat([df_final, augmented_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eM1dyEZhoCY"
   },
   "outputs": [],
   "source": [
    "concatenated_df=concatenated_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "m2Ad8kxJiccK",
    "outputId": "902bfd63-5bab-4cd4-a741-1f8f4befc627"
   },
   "outputs": [],
   "source": [
    "concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qiTuuSwpGBvw",
    "outputId": "1ad72e44-6187-489e-aaff-eda321d7d569"
   },
   "outputs": [],
   "source": [
    "# Modify the extract_features function to accept audio data, sample_rate, and path\n",
    "def extract_features(audio, sample_rate, path):\n",
    "    result = []\n",
    "\n",
    "    # ZCR\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n",
    "    result.extend(zcr)\n",
    "\n",
    "    # Chroma_stft\n",
    "    stft = np.abs(librosa.stft(audio))\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "    result.extend(chroma_stft)\n",
    "\n",
    "    # MFCC\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate).T, axis=0)\n",
    "    result.extend(mfcc)\n",
    "\n",
    "    # Root Mean Square Value\n",
    "    rms = np.mean(librosa.feature.rms(y=audio).T, axis=0)\n",
    "    result.extend(rms)\n",
    "\n",
    "    # MelSpectogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=audio, sr=sample_rate).T, axis=0)\n",
    "    result.extend(mel)\n",
    "\n",
    "    # Add the 'Path' to the result\n",
    "    result.append(path)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Modify the extract_features_from_df function to pass 'path' to extract_features\n",
    "def extract_features_from_df(df):\n",
    "    features_list = []\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting Features\"):\n",
    "        path = row['Path']\n",
    "        emotion = row['Emotion']\n",
    "\n",
    "        try:\n",
    "            audio, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "            features = extract_features(audio, sample_rate, path)\n",
    "            features_list.append([emotion] + list(features))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {path}: {str(e)}\")\n",
    "\n",
    "    # Create a DataFrame from the extracted features\n",
    "    df_features = pd.DataFrame(features_list, columns=['Emotion'] + list(range(1, len(features_list[0]))))\n",
    "\n",
    "    return df_features\n",
    "\n",
    "# Example usage\n",
    "df_features = extract_features_from_df(concatenated_df)\n",
    "print(df_features.head())  # Display the extracted features with 'Path'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-isEpJKnp-QQ",
    "outputId": "01480f48-0ee8-471e-c913-665921a9a113"
   },
   "outputs": [],
   "source": [
    "# Rename the 163rd column to \"Path\"\n",
    "df_features.rename(columns={163: \"Path\"}, inplace=True)\n",
    "\n",
    "# Example usage\n",
    "print(df_features.head())  # Display the DataFrame with the renamed column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pE-Efe_-njyj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate df_final and df_features based on the common columns 'Emotion' and 'Filename'\n",
    "final_df = pd.merge(concatenated_df, df_features, on=['Emotion', 'Path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "OBFy4pL-j3WW",
    "outputId": "38e9d331-0b0a-40d2-8c07-0c0341a1a082"
   },
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-UymhRgkICG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "final_df.to_csv(\"./final-df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ucfpsk93srD7",
    "outputId": "610d463a-dc65-4119-b817-f3435b62ca1d"
   },
   "outputs": [],
   "source": [
    "final_df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2d4Akpls_yh"
   },
   "outputs": [],
   "source": [
    "final_df1=final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVAjkfnsvwrW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "selected_columns = ['Filename', 'Emotion', 'Gender', 'Dataset', 'Path']  # Replace with your column names\n",
    "categorical_df=final_df1[selected_columns]\n",
    "# Drop non-feature columns\n",
    "features_df = final_df1.drop(['Filename', 'Emotion', 'Gender', 'Dataset', 'Path'], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "id": "5-UWKsPSwqbH",
    "outputId": "1dc582a2-3394-4a93-d0c1-63a370b1c9a9"
   },
   "outputs": [],
   "source": [
    "categorical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjRw86j_wRUa"
   },
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Perform row-wise Z-score normalization\n",
    "# We transpose the dataframe, apply scaling, and then transpose it back\n",
    "scaled_features = scaler.fit_transform(features_df.T).T\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_features_df = pd.DataFrame(scaled_features, columns=features_df.columns)\n",
    "\n",
    "final_combined_df = pd.concat([categorical_df, scaled_features_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvf-Q-uexe_7"
   },
   "outputs": [],
   "source": [
    "final_combined_df.to_csv(\"final-combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmKFltU7yJ2u"
   },
   "outputs": [],
   "source": [
    "final_combined_df=final_combined_df.drop(['Dataset'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-2EVjU0yYIp",
    "outputId": "bf118f99-ccf6-4e62-f6eb-08ef55b3768e"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create LabelEncoder objects\n",
    "gender_encoder = LabelEncoder()\n",
    "emotion_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'Gender' and 'Emotion' columns\n",
    "encoded_gender = gender_encoder.fit_transform(final_combined_df['Gender'])\n",
    "encoded_emotion = emotion_encoder.fit_transform(final_combined_df['Emotion'])\n",
    "\n",
    "# Create dictionaries to map encoded values back to original labels\n",
    "gender_mapping = dict(zip(gender_encoder.classes_, gender_encoder.transform(gender_encoder.classes_)))\n",
    "emotion_mapping = dict(zip(emotion_encoder.classes_, emotion_encoder.transform(emotion_encoder.classes_)))\n",
    "\n",
    "# Display the mappings\n",
    "gender_mapping, emotion_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fn5-5yv0zeWe"
   },
   "outputs": [],
   "source": [
    "# Applying LabelEncoder on the 'Gender' and 'Emotion' columns\n",
    "final_combined_df['Gender'] = gender_encoder.transform(final_combined_df['Gender'])\n",
    "final_combined_df['Emotion'] = emotion_encoder.transform(final_combined_df['Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zASYh8VIB6Jv"
   },
   "outputs": [],
   "source": [
    "df_3=pd.read_csv(\"final-combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyR3Ebv0CBHv"
   },
   "outputs": [],
   "source": [
    "df_3.drop(['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "7hG8VxdHCE01",
    "outputId": "0e7f68d1-1632-4322-e533-0b729bb577c9"
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of emotions\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_3['Emotion'].value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title('Emotion Distribution in the Dataset')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEItoYQB1-Rd"
   },
   "outputs": [],
   "source": [
    "final_combined_df.to_csv(\"encoded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbTMGqcW2Fsc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"encoded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrNNWZz13EFC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCe9mN_A3EZj"
   },
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "T-xI8FYj11o_",
    "outputId": "be6099b0-8c65-4e44-a390-d89aafe338b1"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "GWGiV0bjuA2f",
    "outputId": "7160cd3e-501c-43d6-c40e-9b39c0843f68"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "Vl5NGohk2lKY",
    "outputId": "1fac3fa1-a204-496e-ed10-3848fa8e89c0"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Dropping the first column which seems to be an index\n",
    "\n",
    "target = df['Emotion']\n",
    "# Separating out the features and the target variable\n",
    "features = df.drop(['Emotion', 'Gender'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(features)\n",
    "principalDf = pd.DataFrame(data=principalComponents, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Visualizing the PCA\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=\"PC1\", y=\"PC2\", hue=target, data=principalDf, palette=\"deep\")\n",
    "plt.title('PCA of Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH6CsXbv1vKl"
   },
   "source": [
    "The scatter plot displays the data after applying PCA, reducing the dimensionality to two principal components. This plot shows how the samples are distributed in the transformed space, with colors still representing the 'Emotion' category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "RWxOM63V0y3x",
    "outputId": "8c97f243-9b01-4dc8-c868-4c858e1ccaf9"
   },
   "outputs": [],
   "source": [
    "# Computing the correlation heatmap for the original dataset (first 20 features)\n",
    "original_corr_matrix = features.iloc[:, :20].corr()\n",
    "\n",
    "# Extracting the first few principal components from the PCA (let's take the first 20 for comparison)\n",
    "principal_components = pca_full.transform(features_standardized)[:, :20]\n",
    "principal_components_df = pd.DataFrame(principal_components, columns=[f'PC{i+1}' for i in range(20)])\n",
    "\n",
    "# Computing the correlation matrix for the PCA-transformed dataset\n",
    "pca_corr_matrix = principal_components_df.corr()\n",
    "\n",
    "# Plotting the correlation heatmaps\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
    "\n",
    "# Heatmap for the original dataset\n",
    "sns.heatmap(original_corr_matrix, ax=axes[0], annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "axes[0].set_title('Correlation Heatmap of Original Dataset (First 20 Features)')\n",
    "\n",
    "# Heatmap for the PCA-transformed dataset\n",
    "sns.heatmap(pca_corr_matrix, ax=axes[1], annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "axes[1].set_title('Correlation Heatmap of PCA-Transformed Dataset (First 20 PCs)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJA6wg2F1RQC"
   },
   "source": [
    "The comparison between these two heatmaps illustrates the effect of PCA in decorrelating the features and transforming the data into a set of linearly uncorrelated variables (principal components). This transformation is a key aspect of PCA, helping in dimensionality reduction while retaining as much variance as possible from the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYGfx6XN1Qz6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "nPH9jABk0No7",
    "outputId": "d7cf1ebc-a7f1-4d11-8ea0-952f502b874b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Re-standardizing the features as the previous variable was lost due to reset\n",
    "\n",
    "# Re-applying PCA with all components to compute the variance explained\n",
    "pca_full = PCA()\n",
    "pca_full.fit(features_standardized)\n",
    "\n",
    "# Calculating the cumulative variance explained by the principal components\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Plotting the cumulative variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cumulative_variance, marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance by PCA Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcPk7NqP1lbf"
   },
   "source": [
    "This visualization provides a clear picture of the trade-off between the number of components and the amount of information (variance) retained from the original dataset. For instance, if the curve flattens out quickly, it indicates that most of the variance can be captured by a relatively small number of components. This would imply that PCA has effectively reduced the dimensionality of the data while retaining most of its information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "2j3hp4s-xoMR",
    "outputId": "7ae2042b-599e-4cf8-c756-c1a3febd10a1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "model_without_reg = LinearRegression()\n",
    "model_without_reg.fit(features, target)\n",
    "coefficients_without_reg = model_without_reg.coef_\n",
    "\n",
    "# You can adjust the alpha value as needed\n",
    "model_with_reg = Ridge(alpha=1.0)\n",
    "model_with_reg.fit(features, target)\n",
    "coefficients_with_reg = model_with_reg.coef_\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Coefficients without regularization\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(coefficients_without_reg, color='blue')\n",
    "plt.title('Coefficients without L2 Regularization')\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.ylabel('Coefficient Value')\n",
    "\n",
    "# Coefficients with regularization\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(coefficients_with_reg, color='red')\n",
    "plt.title('Coefficients with L2 Regularization')\n",
    "plt.xlabel('Coefficient Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulD6P4Lr47bQ"
   },
   "source": [
    "**Without Regularization:** The coefficients in a standard linear regression model can have large magnitudes. This is because linear regression aims to minimize the residual sum of squares (RSS) without any constraints on the size of the coefficients. As a result, the model might fit the training data very well, including the noise, leading to overfitting.\n",
    "**With L2 Regularization:** Ridge Regression adds a regularization term to the RSS minimization. This term penalizes large coefficients, effectively shrinking their magnitudes. The primary goal is to balance the fit of the model with the complexity (magnitude of coefficients), leading to better generalization and reduced overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cPnAfzg5EeX"
   },
   "source": [
    "**Without Regularization:** Models without regularization can become overly complex, capturing noise in the training data, which hinders their performance on unseen data (overfitting).\n",
    "\n",
    "**With L2 Regularization:** Regularization techniques like Ridge Regression help in reducing the complexity of the model. By constraining the coefficient magnitudes, they ensure that the model does not overly fit the noise in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "yTCp7Gt3746p",
    "outputId": "4d72496a-ca52-40f1-c089-6485240f9666"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "id": "GLyaVevwuSTg",
    "outputId": "869e18cc-3d51-4088-d57a-13f4a4bd6e5e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setting up the figure for multiple subplots\n",
    "plt.figure(figsize=(20, 18))\n",
    "\n",
    "# 1. Emotion Distribution\n",
    "plt.subplot(3, 2, 1)\n",
    "sns.countplot(x='Emotion', data=df, palette='viridis')\n",
    "plt.title('Distribution of Emotions')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 2. Gender Distribution\n",
    "plt.subplot(3, 2, 2)\n",
    "gender_counts = df['Gender'].value_counts()\n",
    "plt.pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', startangle=140, colors=['skyblue', 'salmon'])\n",
    "plt.title('Gender Distribution')\n",
    "\n",
    "# 3. Emotion by Gender\n",
    "plt.subplot(3, 2, 3)\n",
    "sns.countplot(x='Emotion', hue='Gender', data=df, palette='pastel')\n",
    "plt.title('Emotion Distribution by Gender')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "\n",
    "\n",
    "# 5. Feature Analysis (plotting a few features as an example)\n",
    "features_to_plot = [str(i) for i in range(15, 20)]  # Selecting first 5 features for demonstration\n",
    "feature_data = df[features_to_plot + ['Emotion']]\n",
    "plt.subplot(3, 2, 5)\n",
    "sns.boxplot(data=feature_data, palette='coolwarm')\n",
    "plt.title('Boxplot of Selected Features')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "LG9YuLyh-ym1",
    "outputId": "b374460c-1271-401b-cb4a-47d4e032520d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Setting up the figure for additional subplots\n",
    "plt.figure(figsize=(20, 18))\n",
    "\n",
    "\n",
    "\n",
    "# 7. Average Feature Values for Each Emotion\n",
    "# Calculating the mean of features for each emotion\n",
    "mean_features_by_emotion = df.groupby('Emotion')[[str(i) for i in range(1, 163)]].mean()\n",
    "plt.subplot(3, 2, 2)\n",
    "sns.heatmap(mean_features_by_emotion, cmap='coolwarm')\n",
    "plt.title('Average Feature Values for Each Emotion')\n",
    "\n",
    "# 8. Average Feature Values for Each Gender\n",
    "# Calculating the mean of features for each gender\n",
    "mean_features_by_gender = df.groupby('Gender')[[str(i) for i in range(1, 163)]].mean()\n",
    "plt.subplot(3, 2, 3)\n",
    "sns.heatmap(mean_features_by_gender, cmap='Blues')\n",
    "plt.title('Average Feature Values for Each Gender')\n",
    "\n",
    "# 9. Feature Correlation Heatmap\n",
    "# Selecting a subset of features for correlation analysis\n",
    "selected_features = [str(i) for i in range(1, 21)]  # First 20 features\n",
    "correlation_matrix = df[selected_features].corr()\n",
    "plt.subplot(3, 2, 4)\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='viridis')\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "9YKdQ1vO_4Ok",
    "outputId": "4d3c8c4d-57a7-4eff-9ded-a3e57dd7fde1"
   },
   "outputs": [],
   "source": [
    "# For further analysis, we will look at the correlation between features and create graphs to understand these relationships better.\n",
    "\n",
    "# Full feature correlation matrix\n",
    "full_feature_correlation = df[[str(i) for i in range(1, 163)]].corr()\n",
    "\n",
    "# Setting up the figure for additional subplots\n",
    "plt.figure(figsize=(20, 18))\n",
    "\n",
    "# 10. Full Feature Correlation Heatmap\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(full_feature_correlation, cmap='RdBu', vmin=-1, vmax=1)\n",
    "plt.title('Full Feature Correlation Heatmap')\n",
    "\n",
    "# 11. Distribution of Correlation Coefficients\n",
    "# Flattening the correlation matrix and filtering out self-correlations (correlation of a feature with itself)\n",
    "correlation_values = full_feature_correlation.values.flatten()\n",
    "correlation_values = correlation_values[correlation_values != 1]  # Remove self-correlations\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(correlation_values, bins=30, kde=True, color='green')\n",
    "plt.title('Distribution of Correlation Coefficients')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWVgWt1bUJr3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "RsPY9PqC2ier",
    "outputId": "1f4ea8a7-92a9-4d7a-ac7d-6a174bfcd832"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the sizes of each dataset in percentages\n",
    "total_size = len(final_df)\n",
    "train_size_percentage = len(train_set) / total_size * 100\n",
    "validation_size_percentage = len(validation_set) / total_size * 100\n",
    "test_size_percentage = len(test_set) / total_size * 100\n",
    "\n",
    "# Data for plotting\n",
    "sizes = [train_size_percentage, validation_size_percentage, test_size_percentage]\n",
    "labels = ['Training', 'Validation', 'Test']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, sizes, color=['blue', 'green', 'red'])\n",
    "plt.xlabel('Dataset Type')\n",
    "plt.ylabel('Percentage of Total Dataset (%)')\n",
    "plt.title('Distribution of Samples in Training, Validation, and Test Datasets')\n",
    "plt.ylim(0, 100)  # Set y-axis limit to 100% for clarity\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KR_t3rvx3t8E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq7Oz56Gj2xo"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wd4IZfHz5ZkT",
    "outputId": "ff0d43ac-5ef8-42d3-fb99-b6d238b117b1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the features (X) and labels (y)\n",
    "X = df_final['Path']  # Path to audio files\n",
    "y = df_final['Emotion']  # Emotion labels\n",
    "\n",
    "# Split the data into train, test, and validation sets\n",
    "# Here, we'll split the data into 70% train, 15% validation, and 15% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the sizes of the resulting sets\n",
    "print(f\"Train set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
